好的，這是一份根據您的最新需求（版本更新、精簡內容、增加 SRT 格式時間戳及講者辨識）修改後的產品需求文件 **v1.3**。

我已為您移除冗餘部分、更新了版本號，並將新的功能要求整合至核心功能、系統架構和 API 規格中。

-----

## **產品需求文件 (PRD): "EchoChat-Desktop" v1.3**

### **1. 文件資訊**

  * **專案名稱:** EchoChat-Desktop
  * **文件版本:** **1.3**
  * **變更日期:** 2025年7月10日
  * **核心變更:**
      * 版本號更新為 1.3。
      * 新增 STT 供應商選擇。
      * 優化 LLM 逐字稿情境提示。
      * **新增 STT 回傳格式要求 (類 SRT 時間戳與講者辨識)。**

### **2. 核心功能與範疇 (Core Features & Scope)**

#### **2.1. 即時語音轉錄 (Real-time Speech-to-Text)**

1.  **STT 供應商選擇 (新增功能):**

      * **UI 變更**: 在 UI 新增一個下拉式選單，用於選擇語音轉文字供應商 (初始選項為 "Deepgram"，為未來擴充做準備)。
      * **後端邏輯**: WebSocket 連線需根據前端選擇的 STT 供應商，將音訊流導向對應的處理路徑。

2.  **轉錄輸出規格 (核心變更):**

      * **時間戳記格式:** STT 服務回傳給前端的資料，**必須包含每個語句片段的開始與結束時間戳**，格式應類似 SRT (SubRip Text) 字幕，以便未來進行回放或對應操作。
      * **講者辨識 (Speaker Diarization):** STT 服務需啟用講者辨識功能。回傳的資料中，需明確標示出每個語句片段是由哪一位講者發言的 (例如：`講者 A`, `講者 B`)。

#### **2.2. AI 互動體驗**

1.  **動態 AI 角色設定與情境感知提示:**

      * **後端邏輯**: 在處理聊天請求時，自動在使用者自訂的 System Prompt 前，加上一段固定的前綴：`「注意：接下來的內容是一份由語音即時轉錄生成的逐字稿，其中可能包含錯別字或語法不通順的地方。請在理解和分析時將這個因素考慮進去。」`這能讓 LLM 對輸入內容有更準確的預期，提升回答品質。

2.  **前端模型供應商選擇:**

      * [cite\_start]**LLM Provider 選擇**: UI 應提供下拉式選單，用於選擇語言模型供應商 (如 "OpenAI", "Gemini") [cite: 144, 145]。

### **3. 技術棧 (Technology Stack)**

  * **桌面應用框架:** Tauri (核心為 Rust)
  * **前端:** Next.js, TypeScript, Tailwind CSS
  * **後端:** FastAPI, Python, Docker
  * **第三方服務:**
      * **語音轉文字 (STT):** **可插拔模組**，初始支援 **Deepgram**。需啟用其 **時間戳 (Timestamp) 與講者辨識 (Diarization)** 功能。
      * **大型語言模型 (LLM):** **可插拔模組**，支援 **OpenAI / Google Gemini**。

### **4. 系統架構與資料流 (System Architecture & Data Flow)**

**主要變更點：**

  * 前端 WebSocket 將使用者選擇的 STT Provider 作為參數傳遞。
  * 後端 WebSocket 端點根據此參數動態選擇 STT 服務。
  * **STT 服務回傳的不再是純文字流，而是包含時間戳和講者標籤的結構化 JSON 資料。**
  * 後端 Chat API 在組合提示時，自動加入「逐字稿情境」的固定前綴。

<!-- end list -->

```mermaid
graph TD
    subgraph Desktop App (Tauri + Next.js UI)
        A[使用者] --> B[UI Components];
        B -- "設定AI角色, 選擇LLM/STT Provider" --> C[App State];
        
        subgraph Tauri Core (Rust)
            D[System Audio Capture] -- "Audio Bytes" --> E[Tauri-JS Bridge];
        end
        
        E -- "Audio Bytes" --> F[WebSocket Client];
        B -- "聊天提問 (含AI角色/LLM設定)" --> G[REST API Client];
        %% 修改點：接收的資料結構已改變
        F -- "接收結構化轉錄稿 (含時間/講者)" --> B;
        G -- "接收AI回覆" --> B;
    end

    subgraph Server (FastAPI on Docker)
        %% WebSocket 路徑增加 STT Provider 參數
        H[WebSocket Endpoint\n/ws/stream?stt_provider=deepgram] <--> F;
        %% 修改點：後端需啟用講者辨識
        H -- "Audio Bytes" --> I[STT Service Router\n(啟用講者辨識)];
        I -- "Selects & Calls" --> J[STT Adapter\n(Deepgram)];
        J -- "Proxy Stream" --> K((Deepgram API));
        %% 修改點：回傳的是包含更多資訊的 JSON
        K -- "Structured Transcript JSON\n(Timestamp, Speaker...)" --> J;
        J -- "Formatted JSON" --> H;

        L[REST API Endpoint\n/api/chat/stream] <--> G;
        L -- "Prepends 'Transcript Context' Prompt" --> M[LLM Service Router];
        M -- "(Context, Query, Full Prompt, LLM Provider)" --> N[LLM Adapter\n(Gemini/OpenAI)];
        N -- "API Call" --> O((LLM Service API));
        O -- "Completion Stream" --> N;
        N -- "Formatted Response Stream" --> L;
    end
```

### **5. API 規格變更 (API Specification)**

#### **WebSocket 連線與回傳**

1.  **連線 URL:**

      * 前端與後端建立 WebSocket 連線的 URL 格式需包含 STT 供應商資訊。
      * **URL 格式:** `wss://<your-domain>/ws/stream?stt_provider=<provider_name>`
      * **範例:** `wss://echochat.example.com/ws/stream?stt_provider=deepgram`

2.  **訊息格式 (Server -\> Client):**

      * [cite\_start]伺服器回傳給客戶端的訊息格式應為 JSON。這將取代原有的 `TranscriptResponse` 介面 [cite: 388]。

      * **新的 `TranscriptResponse` 結構建議如下：**

        ```typescript
        // 位於: src/types/index.ts

        // 代表一個有時間和講者資訊的語句片段
        export interface TranscriptSegment {
          text: string;
          speaker: string; // e.g., "Speaker A", "Speaker B"
          start_time: number; // 單位：秒
          end_time: number;   // 單位：秒
          confidence?: number;
        }

        // WebSocket 回傳的完整資料結構
        export interface TranscriptResponse {
          is_final: boolean; // 標示是否為最終結果
          segments: TranscriptSegment[]; // 包含一個或多個語句片段
        }
        ```

#### **REST API (Chat Completion)**

[cite\_start]**`POST /api/chat/stream`** 的請求模型 `ChatRequest` 保持不變 [cite: 126, 390]。關於「逐字稿情境提示」的邏輯應在**後端內部實作**，對前端保持透明。