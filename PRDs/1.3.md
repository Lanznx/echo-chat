好的，這兩個都是非常棒的補充，能顯著提升產品的靈活性和最終 AI 輸出的品質。

將「可選擇的語音轉文字 (STT) 供應商」和「對 LLM 的逐字稿情境提示」這兩點加入後，整個產品的架構會更加完整和專業。

我已經將這些新想法整合進先前的 PRD，產生了更新的 v2.1 版本。

-----

## **產品需求文件 (PRD): "EchoChat-Desktop" v2.1**

### **1. 文件資訊**

  * **專案名稱:** EchoChat-Desktop
  * **文件版本:** **2.1**
  * **變更日期:** 2025年7月9日
  * **核心變更:** 新增 STT 供應商選擇、優化 LLM 提示情境。

### **2. 專案總覽與願景 (Project Overview & Vision)**

*(此部分與 v2.0 相同，保持不變)*

### **3. 目標使用者 (Target Audience)**

*(此部分與 v2.0 相同，保持不變)*

### **4. 核心功能與範疇 (Core Features & Scope)**

#### **4.1. 桌面端整合與音訊來源擴增 (Tauri & Audio Source)**

*(此部分與 v2.0 相同，保持不變)*

1.  **Tauri 框架整合**
2.  **系統音訊擷取**

#### **4.2. AI 互動體驗升級**

1.  **動態 AI 角色設定與情境感知提示 (Context-Aware Prompting):**
      * **UI 變更**: 在前端設定區塊中，維持兩個輸入框：
          * **AI 角色 (System Prompt):** 讓使用者自定義 AI 的系統提示。
          * **我的角色 (User Role):** 讓使用者能定義自己的身份。
      * **後端邏輯 (核心變更)**: 在後端處理聊天請求時，**自動在使用者自訂的 System Prompt 前，加上一段固定的前綴**。這段前綴的內容為：`「注意：接下來的內容是一份由語音即時轉錄生成的逐字稿，其中可能包含錯別字或語法不通順的地方。請在理解和分析時將這個因素考慮進去。」` 這樣能讓 LLM 對輸入內容有更準確的預期，從而提供更相關、更寬容的回答。
2.  **前端模型供應商選擇:**
      * **LLM Provider 選擇 (原有功能)**: 在 UI 新增一個下拉式選單，用於選擇語言模型供應商 (如 "OpenAI", "Gemini")。
      * **STT Provider 選擇 (新增功能)**: **在 UI 新增另一個下拉式選單**，用於選擇語音轉文字供應商 (初始選項可為 "Deepgram"，為未來擴充做準備)。這個選擇將決定前端 WebSocket 要連接到後端的哪個處理路徑。

#### **4.3. 部署與分發 (Deployment & Distribution)**

*(此部分與 v2.0 相同，保持不變)*

1.  **後端部署 (Docker Compose)**
2.  **桌面應用程式分發 (Tauri Build + GitHub Actions)**

### **5. 技術棧 (Technology Stack)**

  * **桌面應用框架:** Tauri (核心為 Rust)
  * **前端:** Next.js 14+, TypeScript, Tailwind CSS
  * **後端:** FastAPI, Python 3.10+, Docker
  * **第三方服務:**
      * **語音轉文字 (STT):** **可插拔模組**，初始支援 **Deepgram**，未來可擴充。
      * **大型語言模型 (LLM):** **可插拔模組**，支援 **OpenAI / Google Gemini**。

### **6. 系統架構與資料流 (System Architecture & Data Flow)**

**主要變更點：**

  * 前端在建立 WebSocket 連線時，需將使用者選擇的 STT Provider 作為參數傳遞。
  * 後端 WebSocket 端點需根據此參數，動態選擇對應的 STT 服務進行串流。
  * 後端 Chat API 在組合最終提示時，需自動加入「逐字稿情境」的固定前綴。

<!-- end list -->

```mermaid
graph TD
    subgraph Desktop App (Tauri + Next.js UI)
        A[使用者] --> B[UI Components];
        B -- "設定AI角色, 選擇LLM/STT Provider" --> C[App State];
        
        subgraph Tauri Core (Rust)
            D[System Audio Capture] -- "Audio Bytes" --> E[Tauri-JS Bridge];
        end
        
        E -- "Audio Bytes" --> F[WebSocket Client];
        B -- "聊天提問 (含AI角色/LLM設定)" --> G[REST API Client];
        F -- "接收轉錄稿" --> B;
        G -- "接收AI回覆" --> B;
    end

    subgraph Server (FastAPI on Docker)
        %% WebSocket 路徑增加 STT Provider 參數
        H[WebSocket Endpoint\n/ws/stream?stt_provider=deepgram] <--> F;
        H -- "Audio Bytes" --> I[STT Service Router];
        I -- "Selects & Calls" --> J[STT Adapter\n(Deepgram)];
        J -- "Proxy Stream" --> K((Deepgram API));
        K -- "Transcript JSON" --> J;
        J -- "Formatted Transcript" --> H;

        L[REST API Endpoint\n/api/chat/completion] <--> G;
        L -- "Prepends 'Transcript Context' Prompt" --> M[LLM Service Router];
        M -- "(Context, Query, Full Prompt, LLM Provider)" --> N[LLM Adapter\n(Gemini/OpenAI)];
        N -- "API Call" --> O((LLM Service API));
        O -- "Completion" --> N;
        N -- "Formatted Response" --> L;
    end
```

### **7. API 規格變更 (API Specification)**

#### **WebSocket 連線**

前端與後端建立 WebSocket 連線的 URL 格式需包含 STT 供應商資訊：

  * **URL 格式:** `wss://<your-domain>/ws/stream?stt_provider=<provider_name>`
  * **範例:** `wss://echochat.example.com/ws/stream?stt_provider=deepgram`

後端 WebSocket 端點需要解析這個 `stt_provider` 查詢參數，並將音訊流導向對應的 STT 服務。

#### **REST API (Chat Completion)**

**`POST /api/chat/completion`** 的請求模型 **`ChatRequest`** 保持不變。關於「逐字稿情境提示」的邏輯應在**後端內部實作**，對前端保持透明。

**後端處理流程範例:**

```python
# 示意程式碼 (backend)
@app.post("/api/chat/completion")
async def handle_chat(request: ChatRequest):
    # 固定的情境提示前綴
    transcript_context_prompt = "注意：接下來的內容是一份由語音即時轉錄生成的逐字稿..."

    # 組合最終的 System Prompt
    final_system_prompt = f"{transcript_context_prompt}\n\n{request.system_prompt or ''}"
    
    # 根據 request.provider 選擇 LLM 服務
    llm_service = get_llm_service(request.provider)
    
    # 使用 final_system_prompt 呼叫 LLM
    response = await llm_service.generate(
        context=request.context,
        query=request.query,
        system_prompt=final_system_prompt,
        # ... 其他參數
    )
    
    return response
```

### **8. 暫緩功能 (Out of Scope for v2.1)**

*(此部分與 v2.0 相同，保持不變)*

  * 使用者帳號系統
  * 非即時檔案上傳轉錄